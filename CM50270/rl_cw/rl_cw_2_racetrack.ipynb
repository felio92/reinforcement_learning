{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf79fe83940d6b04684c65d2d5dac439",
     "grade": false,
     "grade_id": "cell-b0b3b6b4f891b031",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# CM50270 Reinforcement Learning\n",
    "## Coursework Part 2: Racetrack\n",
    "\n",
    "In this exercise, you will implement and compare the performance of three reinforcement learning algorithms: On-Policy First-Visit Monte-Carlo Control, Sarsa, and Q-Learning.\n",
    "\n",
    "**Total number of marks:** 30 marks.\n",
    "\n",
    "**What to submit:** Your completed Jupyter notebook (.ipynb file) which should include all of your source code. Please do not change the file name or compress/zip your submission. Please do not include any identifying information on the files you submit. This coursework will be **marked anonymously**.\n",
    "\n",
    "**Where to submit:** CM50270 Moodle Page.\n",
    "\n",
    "You are required to **work individually**. You are welcome to discuss ideas with others but you must design your own implementation and write your own code.\n",
    "\n",
    "**Do not plagiarise**. Plagiarism is a serious academic offence. For details on what plagiarism is and how to avoid it, please visit the following webpage: http://www.bath.ac.uk/library/help/infoguides/plagiarism.html\n",
    "\n",
    "If you are asked to use specific variable names, data-types, function signatures and notebook cells, **please ensure that you follow these instructions**. Not doing so will cause the automarker to reject your work, and will assign you a score of zero for that question. **If the automarker rejects your work because you have not followed the instructions, you may not get any credit for your work**.\n",
    "\n",
    "Please remember to **save your work regularly**.\n",
    "\n",
    "Please be sure to **restart the kernel and run your code from start-to-finish** (Kernel â†’ Restart & Run All) before submitting your notebook. Otherwise, you may not be aware that you are using variables in memory that you have deleted.\n",
    "\n",
    "Your total runtime must be less than **5 minutes** on the University's computers in 10W 0.02 and that **written answer length limits** are adhered to. Otherwise, you may not get credit for your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fed9f0883ae0a1e20a949baba0f02db5",
     "grade": false,
     "grade_id": "cell-e86e35a4b405ff32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## The Racetrack Environment\n",
    "We have implemented a custom environment called \"Racetrack\" for you to use during this piece of coursework. It is inspired by the environment described in the course textbook (Reinforcement Learning, Sutton & Barto, 2018, Exercise 5.12) but not exactly the same.\n",
    "\n",
    "### Environment Description\n",
    "Consider driving a race car around a turn on a racetrack. In order to complete the race as quickly as possible, you would want to drive as fast as you can but, to avoid running off the track, you must slow down while turning.\n",
    "\n",
    "In our simplified racetrack environment, the agent is at one of a discrete set of grid positions. The agent also has a discrete speed in two directions, $x$ and $y$. So the state is represented as follows:\n",
    "$$(\\text{position}_y, \\text{position}_x, \\text{velocity}_y, \\text{velocity}_x)$$\n",
    "\n",
    "The agent collects a reward of -1 at each time step, an additional -10 for leaving the track (i.e., ending up on a black grid square in the figure below), and an additional +10 for reaching the finish line (any of the red grid squares). The agent starts each episode in a randomly selected  grid-square on the starting line (green grid squares) with a speed of zero in both directions. At each time step, the agent can change its speed in both directions. Each speed can be changed by +1, -1 or 0, giving a total of nine actions. For example, the agent may increase its speed in the $x$ direction by -1 and its speed in the $y$ direction by +1.\n",
    "\n",
    "<img src=\"images/track_big.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "The agent's next state is determined by its current grid square, its current speed in two directions, and the changes it  makes to its speed in the two directions. This environment is stochastic. When the agent tries to change its speed, no change occurs (in either direction) with probability 0.1. In other words, 10% of the time, the agent's action is ignored and the car's speed remains the same in both directions.\n",
    "\n",
    "If the agent leaves the track, it is returned to a random start grid-square and has its speed set to zero in both directions; the episode continues. An episode ends only when the agent transitions to a goal grid-square.\n",
    "\n",
    "\n",
    "\n",
    "### Environment Implementation\n",
    "We have implemented the above environment in the `racetrack_env.py` file, for you to use in this coursework. Please use this implementation instead of writing your own, and please do not modify the environment.\n",
    "\n",
    "We provide a `RacetrackEnv` class for your agents to interact with. The class has the following methods:\n",
    "- **`reset()`** - this method initialises the environment, chooses a random starting state, and returns it. This method should be called before the start of every episode.\n",
    "- **`step(action)`** - this method takes an integer action (more on this later), and executes one time-step in the environment. It returns a tuple containing the next state, the reward collected, and whether the next state is a terminal state.\n",
    "- **`render(sleep_time)`** - this method renders a matplotlib graph representing the environment. It takes an optional float parameter giving the number of seconds to display each time-step. This method is useful for testing and debugging, but should not be used during training since it is *very* slow. **Do not use this method in your final submission**.\n",
    "- **`get_actions()`** - a simple method that returns the available actions in the current state. Always returns a list containing integers in the range [0-8] (more on this later).\n",
    "\n",
    "In our code, states are represented as Python tuples - specifically a tuple of four integers. For example, if the agent is in a grid square with coordinates ($Y = 2$, $X = 3$), and is moving zero cells vertically and one cell horizontally per time-step, the state is represented as `(2, 3, 0, 1)`. Tuples of this kind will be returned by the `reset()` and `step(action)` methods.\n",
    "\n",
    "There are nine actions available to the agent in each state, as described above. However, to simplify your code, we have represented each of the nine actions as an integer in the range [0-8]. The table below shows the index of each action, along with the corresponding changes it will cause to the agent's speed in each direction.\n",
    "\n",
    "<img src=\"images/action_grid.png\" style=\"width: 250px;\"/>\n",
    "\n",
    "For example, taking action 8 will increase the agent's speed in the $x$ direction, but decrease its speed in the $y$ direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e4714ee739adca912176564b3eb00229",
     "grade": false,
     "grade_id": "cell-30ac99abe97e62b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Racetrack Code Example\n",
    "Below, we go through a quick example of using the `RaceTrackEnv` class.\n",
    "\n",
    "First, we import the class, then create a `RaceTrackEnv` object called `env`. We then initialise the environment using the `reset()` method, and take a look at the initial state variable and the result of `plot()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ac22a56ca4687400306302c35b75a91",
     "grade": false,
     "grade_id": "cell-77add459a6f282dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Set random seed to make example reproducable.\n",
    "import numpy as np\n",
    "import random\n",
    "seed = 5\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "from racetrack_env import RacetrackEnv\n",
    "\n",
    "# Instantiate environment object.\n",
    "env = RacetrackEnv()\n",
    "\n",
    "# Initialise/reset environment.\n",
    "state = env.reset()\n",
    "env.render()\n",
    "print(\"Initial State: {}\".format(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf70e6e3c9fe761473c11366c91f40ff",
     "grade": false,
     "grade_id": "cell-b42bead8118e3c9e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see, `reset()` has returned a valid initial state as a four-tuple. The function `plot()` uses the same colour-scheme as described above, but also includes a yellow grid-square to indicate the current position of the agent.\n",
    "\n",
    "Let's make the agent go upward by using `step(1)`, then inspect the result (recall that action `1` increments the agent's vertical speed while leaving the agent's horizontal speed unchanged)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "409bb221e1080a4a02e52db851d6dc86",
     "grade": false,
     "grade_id": "cell-8cb86c18bf331894",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let us increase the agent's vertical speed (action 1).\n",
    "next_state, reward, terminal = env.step(1)\n",
    "env.render()\n",
    "print(\"Next State: {}, Reward: {}, Terminal: {}\".format(next_state, reward, terminal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a68773faf856fac19dc863fe5e3b01f4",
     "grade": false,
     "grade_id": "cell-4f51e890424d0c2d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can see that the agent has moved one square upwards, and now has a positive vertical speed (indicated by the yellow arrow). Let's set up a loop to see what happens if we take the action a few more times, causing it to repeatedly leave the track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50931f73b836a9941366a8e8b67805a2",
     "grade": false,
     "grade_id": "cell-ef8865037a9ebdeb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "num_steps = 50\n",
    "for t in range(num_steps) :\n",
    "    next_state, reward, terminal = env.step(1)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d8d74882a1c8d3d4e26d6a4f82ac47e",
     "grade": false,
     "grade_id": "cell-a21a5643628d8354",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 1: On-Policy MC Control (10 Marks)\n",
    "\n",
    "In this exercise, you will implement an agent which learns to reach a goal state in the racetrack task using On-Policy First-Visit MC Control, the pseudocode for which is reproduced below (Reinforcement Learning, Sutton & Barto, 2018, Section 5.4 p.101).\n",
    "\n",
    "<img src=\"images/mc_control_algo.png\" style=\"width: 650px;\"/>\n",
    "\n",
    "Please produce the following:\n",
    "- A tabular On-Policy First-Visit MC Control agent which learns an optimal policy in the racetrack environment.\n",
    "- An average learning curve. Your learning curve should plot the mean return from many agents as a function of episodes. Please specify how many agents' performances you are averaging in the title of your plot. This should be a dynamic figure that we can produce by running your code. If you wish to use any kind of graph smoothing, please also include an un-smoothed version of your graph, and ensure that your smoothing method does not cause artifacts near the edges of the plot.\n",
    "\n",
    "Please use the following parameter settings:\n",
    "- Step size parameter $\\alpha = 0.2$.\n",
    "- Discount factor $\\gamma = 0.9$.\n",
    "- For your $\\epsilon$-greedy policy, use exploratory action probability $\\epsilon = 0.1$.\n",
    "- Number of training episodes $= 150$.\n",
    "- Number of agents averaged should be at least 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_soft(actions, p_values, state, eps=0.1):\n",
    "    suboptimal_actions = []\n",
    "    optimal_action = None\n",
    "    for a in actions:\n",
    "        if p_values[(state, a)] == eps/9:\n",
    "            suboptimal_actions.append(a)\n",
    "        elif p_values[(state, a)] == 1 - eps + eps/9:\n",
    "            optimal_action = a\n",
    "    if np.random.uniform(0, 1) < eps/9:\n",
    "        action = np.random.choice(suboptimal_actions)\n",
    "    else:\n",
    "        action = optimal_action\n",
    "        if optimal_action==None:\n",
    "            action = np.random.choice(suboptimal_actions)\n",
    "    return action    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_policy_mc(environment, policy, n_agents=20, n_episodes=150, eps=0.1, gamma=0.9):\n",
    "    env=environment\n",
    "    actions = env.get_actions()\n",
    "    returns_array=np.zeros(n_episodes)\n",
    "    value_array=np.zeros(n_episodes)\n",
    "    for agent in range(n_agents):\n",
    "        p_values=defaultdict(lambda:eps/9, {})\n",
    "        q_values=defaultdict(lambda:0, {})\n",
    "        returns=defaultdict(lambda:[],{})\n",
    "        \n",
    "        for episode in range(n_episodes):\n",
    "            state=env.reset()\n",
    "            episode_array=[]\n",
    "            state_action_array=[]\n",
    "            first_occurence=defaultdict(lambda:None,{})\n",
    "            episode_ended=False\n",
    "            step=0\n",
    "            #Generate an episode\n",
    "            while not episode_ended:\n",
    "                action = policy(actions, p_values, state, eps)\n",
    "                next_state, reward, terminal = env.step(action)\n",
    "                episode_array.append((state,action, reward))\n",
    "                state_action_array.append((state,action))\n",
    "                if first_occurence[(state,action)] == None:\n",
    "                    first_occurence[(state,action)] = step\n",
    "                state = next_state\n",
    "                step+=1\n",
    "                if terminal==True:\n",
    "                    episode_ended=True\n",
    "            G=0\n",
    "            value=0\n",
    "            for t in range(len(episode_array)-1, -1, -1 ):\n",
    "                state, action, reward = episode_array[t]\n",
    "                G = gamma*G + reward\n",
    "                value = value + reward\n",
    "                if  t == first_occurence[(state, action)]:\n",
    "                    returns[(state, action)].append(G)\n",
    "                    q_values[(state, action)] = sum(returns[(state, action)])/len(returns[(state, action)])\n",
    "                    q_action = [q_values[(state, a)] for a in actions]\n",
    "                    max_q_value = np.max(q_action)\n",
    "                    optimal_actions = [a for a in actions if q_values[(state, a)]==max_q_value]\n",
    "                    a_prime = np.random.choice(optimal_actions)\n",
    "                    for a in actions:\n",
    "                        if a==a_prime:\n",
    "                            p_values[(state, a)] = 1 - eps + eps/9\n",
    "                        else: \n",
    "                            p_values[(state, a)] = eps/9 \n",
    "    \n",
    "            returns_array[episode] += value/n_agents\n",
    "    return p_values, q_values, returns_array\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_agents=50\n",
    "_, mc_q_values, mc_returns=on_policy_mc(env, epsilon_soft, n_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a642605880bdf1e276228faacc1a83f5",
     "grade": true,
     "grade_id": "cw2_racetrack_mc",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code for Exercise 1 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# You should implement your MC agent and produce your average learning curve.\n",
    "# Do NOT delete this cell.\n",
    "# YOUR CODE HERE\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(\"111\")\n",
    "ax.set_title(\"Learning curve for \"+str(n_agents)+\" agents.\")\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Average returns\")\n",
    "ax.plot(mc_returns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "afaf616bc1f56770fe1e2a642744f28f",
     "grade": false,
     "grade_id": "cell-e74a2cb7d25f13ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2: Sarsa (5 Marks)\n",
    "\n",
    "In this exercise, you will implement an agent which learns to reach a goal state in the racetrack task using the Sarsa algorithm, the pseudocode for which is reproduced below (Reinforcement Learning, Sutton & Barto, 2018, Section 6.4 p.129).\n",
    "\n",
    "<img src=\"images/sarsa_algo.png\" style=\"width: 650px;\"/>\n",
    "\n",
    "Please produce the following:\n",
    "\n",
    "- A tabular sarsa agent which learns an optimal policy in the racetrack environment.\n",
    "- An average learning curve. Your learning curve should plot the mean return from many agents as a function of episodes. Please specify how many agents' performances you are averaging in the title of your plot. This should be a dynamic figure that we can produce by running your code. If you wish to use any kind of graph smoothing, please also include an un-smoothed version of your graph, and ensure that your smoothing method does not cause artifacts near the edges of the plot.\n",
    "\n",
    "\n",
    "Please use the following parameter settings:\n",
    "- Step size parameter $\\alpha = 0.2$.\n",
    "- Discount factor $\\gamma = 0.9$.\n",
    "- For your $\\epsilon$-greedy policy, use exploratory action probability $\\epsilon = 0.1$.\n",
    "- Number of training episodes $= 150$.\n",
    "- Number of agents averaged should be at least 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy(actions, q_values, state, eps=0.1):\n",
    "    q_action = [q_values[(state, a)] for a in actions]\n",
    "    max_q_value = np.max(q_action)\n",
    "    optimal_actions = [a for a in actions if q_values[(state, a)]==max_q_value]\n",
    "    optimal_action = np.random.choice(optimal_actions)\n",
    "    if np.random.uniform()<eps:\n",
    "        action = np.random.choice(np.delete(actions, optimal_action))\n",
    "    else:\n",
    "        action = optimal_action\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_policy_sarsa(environment, policy, n_agents=20, n_episodes=150, eps=0.1, gamma=0.9, alpha=0.2):\n",
    "    returns_array = np.zeros(n_episodes)\n",
    "    value_array = np.zeros(n_episodes)\n",
    "    for agent in range(n_agents):\n",
    "        q_values = defaultdict(lambda:0, {})\n",
    "        actions = env.get_actions()\n",
    "        for episode in range(n_episodes):\n",
    "            state = env.reset()\n",
    "            action =policy(actions, q_values, state, eps)\n",
    "            episode_ended = False\n",
    "            episode_array = []\n",
    "            G = 0\n",
    "            value = 0\n",
    "            while not episode_ended:\n",
    "                next_state, reward, terminal = env.step(action)\n",
    "                episode_array.append((state, action, reward))\n",
    "                next_action = policy(actions, q_values, next_state, eps)\n",
    "                q_current = q_values[(state, action)]\n",
    "                q_next = q_values[(next_state, next_action)]\n",
    "                q_values[(state, action)] = q_current + alpha*(reward + gamma*q_next - q_current)\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                value += reward\n",
    "                if terminal==True:\n",
    "                    episode_ended=True\n",
    "                \n",
    "            returns_array[episode] += value/n_agents\n",
    "    return q_values, returns_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_agents = 50\n",
    "sarsa_q_values, sarsa_returns = on_policy_sarsa(env, eps_greedy, n_agents, n_episodes=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "514e5ce2825ae70db38624a5e7a7b2ca",
     "grade": true,
     "grade_id": "cw2_racetrack_sarsa",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code for Exercise 2 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# You should implement your sarsa agent and produce your average learning curve.\n",
    "# Do NOT delete this cell.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(\"111\")\n",
    "ax.set_title(\"Learning curve for \"+str(n_agents)+\" agents.\", fontsize=12)\n",
    "ax.set_xlabel(\"Episode\", fontsize=12)\n",
    "ax.set_ylabel(\"Average returns\", fontsize=12)\n",
    "ax.plot(sarsa_returns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "658c8893e6e8eec08636a7520e5a5d77",
     "grade": false,
     "grade_id": "cell-b0d45e347090cb02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 3: Q-Learning (5 Marks)\n",
    "\n",
    "In this exercise, you will implement an agent which learns to reach a goal state in the racetrack task using the Q-Learning algorithm, the pseudocode for which is reproduced below (Reinforcement Learning, Sutton & Barto, 2018, Section 6.5 p.131).\n",
    "\n",
    "<img src=\"images/q_learning_algo.png\" style=\"width: 650px;\"/>\n",
    "\n",
    "Please produce the following:\n",
    "\n",
    "- A tabular Q-Learning agent which learns an optimal policy in the racetrack environment.\n",
    "- An average learning curve. Your learning curve should plot the mean return from many agents as a function of episodes. Please specify how many agents' performances you are averaging in the title of your plot. This should be a dynamic figure that we can produce by running your code. If you wish to use any kind of graph smoothing, please also include an un-smoothed version of your graph, and ensure that your smoothing method does not cause artifacts near the edges of the plot.\n",
    "\n",
    "Please use the following parameter settings:\n",
    "- Step size parameter $\\alpha = 0.2$.\n",
    "- Discount factor $\\gamma = 0.9$.\n",
    "- For your $\\epsilon$-greedy policy, use exploratory action probability $\\epsilon = 0.1$.\n",
    "- Number of training episodes $= 150$.\n",
    "- Number of agents averaged should be at least 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_policy_qlearning(environment, policy, n_agents=20, n_episodes=150, eps=0.1, gamma=0.9, alpha=0.2):\n",
    "    returns_array = np.zeros(n_episodes)\n",
    "    value_array = np.zeros(n_episodes)\n",
    "    for agent in range(n_agents):\n",
    "        q_values = defaultdict(lambda:0, {})\n",
    "        actions = env.get_actions()\n",
    "        for episode in range(n_episodes):\n",
    "            state = env.reset()\n",
    "            episode_ended = False\n",
    "            episode_array = []\n",
    "            G = 0\n",
    "            value = 0\n",
    "            while not episode_ended:\n",
    "                action = policy(actions, q_values, state, eps)\n",
    "                next_state, reward, terminal = env.step(action)\n",
    "                episode_array.append((state, action, reward))\n",
    "                q_current = q_values[(state, action)]\n",
    "                q_next = max([q_values[(next_state, a)] for a in actions])\n",
    "                q_values[(state, action)] = q_current + alpha*(reward + gamma*q_next - q_current)\n",
    "                state = next_state\n",
    "                value += reward\n",
    "                if terminal==True:\n",
    "                    episode_ended=True  \n",
    "            returns_array[episode] += value/n_agents\n",
    "    return q_values, returns_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_agents = 50\n",
    "qlearning_q_values, qlearning_returns= off_policy_qlearning(env, eps_greedy, n_agents, n_episodes=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "181fd21096e8386b9ba61526b12da68e",
     "grade": true,
     "grade_id": "cw2_racetrack_q_learning",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code for Exercise 3 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# You should implement your Q-Learning agent and produce your average learning curve.\n",
    "# Do NOT delete this cell.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Please write your code for Exercise 2 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# You should implement your sarsa agent and produce your average learning curve.\n",
    "# Do NOT delete this cell.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(\"111\")\n",
    "ax.set_title(\"Learning curve for \"+str(n_agents)+\" agents.\", fontsize=12)\n",
    "ax.set_xlabel(\"Episode\", fontsize=12)\n",
    "ax.set_ylabel(\"Average returns\", fontsize=12)\n",
    "ax.plot(qlearning_returns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1f9c05036c9cc912512d0cc20784420",
     "grade": false,
     "grade_id": "cell-b5dc1ef13af04d71",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 4: Comparison & Discussion (10 Marks)\n",
    "\n",
    "Please produce a plot which shows the data from your previous three graphs plotted together. Be sure to include a legend, so that it is clear which data series corresponds the performance of which agent.\n",
    "\n",
    "Please note that **you should not re-run your experiments from scratch**. You should re-use your results from the previous exercises.\n",
    "\n",
    "If you wish to use any kind of graph smoothing or cropping when presenting your results, please also include an un-altered version of your graph. Please ensure that any graph smoothing method you use does not cause artifacts near the edges of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab6c6b5aba10608f52d5be76df89575d",
     "grade": true,
     "grade_id": "cw2_racetrack_comparison_graphs",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code for Exercise 4 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# You should produce your combined graph, clearly showing the learning curves of your three agents.\n",
    "# Do NOT delete this cell.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(\"111\")\n",
    "ax.plot(mc_returns, label=\"On-policy MC control\")\n",
    "ax.plot(sarsa_returns, label=\"On-policy SARSA control\")\n",
    "ax.plot(qlearning_returns, label=\"Off-policy Q-learning control\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a93575e344498b22a02bf4e4e1d12e08",
     "grade": false,
     "grade_id": "cell-cad20c789de502f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In eight sentences or fewer, please discuss the following:\n",
    "- The performance of your different agents.\n",
    "- Why each of your agents performed differently.\n",
    "- Explain the differences you saw, and expected to see, between the performances and polices of your Sarsa and Q-Learning agents.\n",
    "- What could be done to improve the performance of your agents?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3095cac1d761b7369b087c66a6715b8d",
     "grade": true,
     "grade_id": "cw2_racetrack_discussion",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "It is notable that the on-policy MC control performed the worst, which is because under MC control the Q-values are only updated off-line, meaning that the policy is only updated once the episode terminates under the current policy, which can take a signficant amount of time and result in it incurring a lot of negative rewards. Due to the use of a soft-epsilon-policy, thus guaranteeing all possible state-action combinations (as episodes go to infinity), the MC control is guaranteed to converge to the optimal policy, which can be seen in the learning curve converging to approx. 0 with increasing number of episodes.\n",
    "\n",
    "The on-policy SARSA and off-policy Q-Learning controls have a significantly better performance than the MC control, which is mainly due to the fact that both methods improve their policy on-line, meaning that the Q-values of state-action pairs are updated as the agent follows it's policy. The use of an epsilon-greedy policy ensures exploration and thus both controls result in a policy approximating the optimal one. Looking at the learning curves of the two, we can see that both of them performing almost identically well, which might be surprising, as SARSA takes the action selection into account, whereas Q-Learning does not. With and epsilon-greedy policy this should lead to Q-Learning learning an optimal policy that leads to it sometimes crashing into the walls of the racetrack. The 10% chance determined by the environment of ignoring a selected action could potentially lead to SARSA learning a similar policy to that produced by Q-Learning.\n",
    "\n",
    "Instead of simply using a one-step look-ahead policy control, using eligibility traces might improve the performance of our agents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
