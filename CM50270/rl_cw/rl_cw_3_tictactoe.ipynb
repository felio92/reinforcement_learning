{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "87c81e84938be381d494b523e04a0fc3",
     "grade": false,
     "grade_id": "cell-7f67b219ed6b3541",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# CM50270 Reinforcement Learning\n",
    "## Coursework Part 3: Tic-Tac-Toe\n",
    "\n",
    "In this piece of coursework, you will implement the game of Tic-Tac-Toe and a Q-Learning agent capable of playing it.\n",
    "\n",
    "**Total number of marks:** 40 marks.\n",
    "\n",
    "**What to submit:** Your completed Jupyter notebook (.ipynb file) which should include all of your source code. Please do not change the file name or compress/zip your submission. Please do not include any identifying information on the files you submit. This coursework will be **marked anonymously**.\n",
    "\n",
    "**Where to submit:** CM50270 Moodle Page.\n",
    "\n",
    "You are required to **work individually**. You are welcome to discuss ideas with others but you must design your own implementation and write your own code.\n",
    "\n",
    "**Do not plagiarise**. Plagiarism is a serious academic offence. For details on what plagiarism is and how to avoid it, please visit the following webpage: http://www.bath.ac.uk/library/help/infoguides/plagiarism.html\n",
    "\n",
    "If you are asked to use specific variable names, data-types, function signatures and notebook cells, **please ensure that you follow these instructions**. Not doing so will cause the automarker to reject your work, and will assign you a score of zero for that question. **If the automarker rejects your work because you have not followed the instructions, you may not get any credit for your work**.\n",
    "\n",
    "Please remember to **save your work regularly**.\n",
    "\n",
    "Please be sure to **restart the kernel and run your code from start-to-finish** (Kernel â†’ Restart & Run All) before submitting your notebook. Otherwise, you may not be aware that you are using variables in memory that you have deleted.\n",
    "\n",
    "Your total runtime must be less than **3 minutes** on the University's computers in 10W 0.02 and that **written answer length limits** are adhered to. Otherwise, you may not get credit for your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Tic-Tac-Toe Environment\n",
    "For this exercise, you should implement the game of [Tic-Tac-Toe](https://en.wikipedia.org/wiki/Tic-tac-toe) (also known as Noughts and Crosses). After you have implemented the environment, you will validate it by letting two random agents play against each other.\n",
    "\n",
    "### The Tic-Tac-Toe Environment\n",
    "\n",
    "Tic-Tac-Toe is a simple game for two players, O and X, who take turns marking cells in a 3x3 grid. The player who succeeds in drawing three instances of their symbol in a horizontal, vertical or diagonal line wins the game. The following example, from [Wikipedia](https://commons.wikimedia.org/wiki/File:Tic-tac-toe-game-1.svg), shows a game that is won by the X player.\n",
    "\n",
    "<img src=\"images/tic-tac-toe_example.svg\" style=\"width: 600px;\"/> \n",
    "\n",
    "### Instructions\n",
    "Implement the game of Tic-Tac-Toe. The first-moving player should be randomly selected at the beginning of each episode. Player X should always be a random agent (i.e. it places an X symbol in a random empty grid-space each time-step). You will implement various agents for player O. Rewards of +1, -1 and 0 should be collected for winning, losing and drawing respectively. All other rewards collected during the game should be 0.\n",
    "\n",
    "Test your Tic-Tac-Toe implementation by letting two random agents play against each other for 100 episodes. Plot the **cumulative rewards** collected by both the O and X agents as a function of the number of episodes.\n",
    "\n",
    "Hint: The X player can be considered to be part of the environment, as it is outside the agent's control.\n",
    "\n",
    "**Exercises 1, 2, and 4 will be marked jointly; you will see this joint score in Exercise 4. You will be given marks for the strength of your implementation and your understanding of the techniques you have implemented. Please note that the standard implementation of Q-learning will receive at most 8 marks from exercise 3 and 0 marks from exercises 1, 2, and 4. This is the most challenging part of your coursework. Precise instructions are given below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gameboard = np.zeros((3, 3))\n",
    "        self.players = [0, 1]\n",
    "        self.first_player = np.random.choice(self.players)\n",
    "   \n",
    "    def reset(self):\n",
    "        self.gameboard = np.zeros((3, 3))\n",
    "        self.first_player = np.random.choice(self.players)\n",
    "        \n",
    "    def is_winning_state(self, state):\n",
    "        hor_win = 3 in np.sum(state, axis=1)\n",
    "        vert_win = 3 in np.sum(state, axis=0)\n",
    "        diag_win = 3 == np.trace(state)\n",
    "        antidiag_win = 3 == np.trace(state[:,-1::-1])\n",
    "        is_win = hor_win or vert_win or diag_win or antidiag_win\n",
    "        return is_win\n",
    "    \n",
    "    def is_losing_state(self, state):\n",
    "        hor_loss = -3 in np.sum(state, axis=1)\n",
    "        vert_loss = -3 in np.sum(state, axis=0)\n",
    "        diag_loss = -3 == np.trace(state)\n",
    "        antidiag_loss = -3 == np.trace(state[:,-1::-1])\n",
    "        is_loss = hor_loss or vert_loss or diag_loss or antidiag_loss\n",
    "        return is_loss\n",
    "        \n",
    "    def is_tie_state(self, state):\n",
    "        if not 0 in state and not (self.is_winning_state(state) or self.is_losing_state(state)):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def is_final_state(self, state):\n",
    "        if not (self.is_losing_state(state) or self.is_winning_state(state) or self.is_tie_state(state)):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    \n",
    "    def get_possible_actions(self, state):\n",
    "        possible_actions = [(i, j) for i in range(3) for j in range(3) if state[i, j]==0]\n",
    "        return possible_actions\n",
    "             \n",
    "    def step(self, state, action, player):\n",
    "        reward = np.array([0, 0])\n",
    "        terminal = False\n",
    "        new_state = np.copy(state)\n",
    "        new_state[action] = (-1)**player\n",
    "        if self.is_winning_state(new_state):\n",
    "            reward = np.array([1, -1])\n",
    "            terminal = True\n",
    "        elif self.is_losing_state(new_state):\n",
    "            reward = np.array([-1, 1])\n",
    "            terminal = True\n",
    "        elif self.is_tie_state(new_state):\n",
    "            terminal = True\n",
    "        return new_state, reward, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random_Agent(object):\n",
    "    def choose_action(self, state, available_actions):\n",
    "        action_index = np.random.randint(0, len(available_actions))\n",
    "        action = available_actions[action_index]\n",
    "        return action\n",
    "    def learn(self, old_state, reward, new_state, action):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_tictactoe(game, agent1, agent2=Random_Agent() ,n_episodes=100):\n",
    "    returns_1 = np.zeros(n_episodes)\n",
    "    returns_2 = np.zeros(n_episodes)\n",
    "    agents = {0: agent1, 1: agent2}\n",
    "    cu_reward_1 = 0\n",
    "    cu_reward_2 = 0\n",
    "    for episode in range(n_episodes):\n",
    "        game_ended = False\n",
    "        first_player = game.first_player\n",
    "        second_player = 1 - first_player\n",
    "        step = 0\n",
    "        while not game_ended:\n",
    "            \n",
    "            #perform step by first player\n",
    "            old_state = np.copy(game.gameboard)\n",
    "            first_available_actions = game.get_possible_actions(old_state)\n",
    "            first_action=agents[first_player].choose_action(old_state, first_available_actions)\n",
    "            first_state, first_reward, first_terminal = game.step(old_state, first_action, first_player)\n",
    "            \n",
    "            #if game has already progressed one or more moves, update second player by using previous move\n",
    "            if step>=1:\n",
    "                agents[second_player].learn(first_state_prev, first_reward[second_player], first_state, second_action)\n",
    "                \n",
    "            #check if game has ended after first player's move\n",
    "            if first_terminal:\n",
    "                game_ended=True\n",
    "                cu_reward_1 += first_reward[0]\n",
    "                cu_reward_2 += first_reward[1] \n",
    "                agents[first_player].learn(old_state, first_reward[first_player], first_state, first_action)\n",
    "                game.reset()\n",
    "                break\n",
    "                \n",
    "            #if not ended, perform second player's move\n",
    "            second_available_actions = game.get_possible_actions(first_state)\n",
    "            second_action = agents[second_player].choose_action(first_state, second_available_actions)\n",
    "            second_state, second_reward, second_terminal = game.step(first_state, second_action, second_player)\n",
    "            \n",
    "            #train first agent\n",
    "            agents[first_player].learn(old_state, second_reward[first_player], second_state, first_action)\n",
    "            \n",
    "            #check if game has ended after second player performed his move\n",
    "            if second_terminal:\n",
    "                game_ended=True\n",
    "                cu_reward_1 += second_reward[0]\n",
    "                cu_reward_2 += second_reward[1] \n",
    "                agents[second_player].learn(first_state, second_reward[second_player], second_state, second_action)\n",
    "                game.reset()\n",
    "                break\n",
    "                \n",
    "            #update gameboard \n",
    "            step+=1\n",
    "            game.gameboard = second_state\n",
    "            first_state_prev = np.copy(first_state)\n",
    "            \n",
    "        returns_1[episode] = cu_reward_1\n",
    "        returns_2[episode] = cu_reward_2\n",
    "                 \n",
    "    return returns_1, returns_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5fba896ce5c0ace5ce0cfcf0253fd0d",
     "grade": true,
     "grade_id": "cw3_tictactoe_environment",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code for Exercise 1 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# You should implement your Tic-Tac-Toe Environment, your random O agent, and produce your cumulative reward plot.\n",
    "# Do NOT delete this cell.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "n_episodes = 100\n",
    "game = TicTacToe()\n",
    "player1 = Random_Agent()\n",
    "episodes = np.arange(0, n_episodes, 1, dtype=int)\n",
    "r1, r2=play_tictactoe(game, player1, n_episodes=n_episodes)\n",
    "\n",
    "fig=plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(\"111\")\n",
    "ax.set_title(\"Cumulative rewards of two random agents playing against another\")\n",
    "ax.plot(episodes, r1, label=\"Random Agent 1\")\n",
    "ax.plot(episodes, r2, label=\"Random Agent 2\")\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Cum. reward\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Q-Learning Agent\n",
    "In this exercise, you will implement an agent which learns an optimal policy for playing Tic-Tac-Toe against a random opponent using Q-Learning. For your reference, the pseudocode for the Q-Learning algorithm is reproduced below (Reinforcement Learning, Sutton & Barto, 2018, Section 6.5 p.131).\n",
    "\n",
    "<img src=\"images/q_learning_algo.png\" style=\"width: 650px;\"/>\n",
    "\n",
    "In order to score high marks in this coursework, you will need to extend your solution beyond a simple Q-Learning agent to achieve more efficient learning (i.e., using fewer interactions with the environment). Ideas for improving your agent will have been discussed in lectures, and can be found in the course textbook (Reinforcement Learning, Sutton & Barto, 2018). However you go about improving your agent, it must still use tabular Q-Learning at its core. \n",
    "\n",
    "Please produce the following:\n",
    "- A Q-Learning agent for the O player, with whatever modifications you believe are reasonable in order to acheieve good performance in the game of Tic-Tac-Toe.\n",
    "- A learning curve for your agent. This learning curve should plot the performance of a single agent, and should be smoothed by taking the mean of the cumulative rewards collected by the agent every 20 episodes. You should not smooth your learning curve in any other way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning_Agent(object):\n",
    "    def __init__(self, game, player, alpha=0.2, gamma=0.9, eps=0.1):\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "        self.gamma = gamma\n",
    "        self.player = player\n",
    "        self.game = game\n",
    "        self.q_values = defaultdict(lambda: 0, {})\n",
    "    \n",
    "    def rotate_90(self, state, action):\n",
    "        s_rot90 = np.array([[state[2-j,i] for j in range(3)] for i in range(3)])\n",
    "        a_rot90 = (2-action[1],action[0])\n",
    "        sa_rot90 = (s_rot90, a_rot90)\n",
    "        return sa_rot90\n",
    "        \n",
    "    def calc_symmetries(self, state, action):\n",
    "        #reflect along the vertical\n",
    "        s_v = state[:, -1::-1]\n",
    "        a_v = (action[0], 2 - action[1])\n",
    "        sa_v = (s_v, a_v)\n",
    "        #reflect along the horizontal\n",
    "        s_h = state[-1::-1,:]\n",
    "        a_h = (2 - action[0], action[1])\n",
    "        sa_h = (s_h, a_h)\n",
    "        #reflect along the left-right diagonal\n",
    "        s_lr = state[-1::-1,-1::-1].transpose()\n",
    "        a_lr = (2-action[1], 2-action[0])\n",
    "        sa_lr = (s_lr, a_lr)\n",
    "        #reflect along the right-left diagonal\n",
    "        s_rl = state.transpose()\n",
    "        a_rl = (action[1], action[0])\n",
    "        sa_rl = (s_rl, a_rl)\n",
    "        #rotate by 90 degrees\n",
    "        sa_rot90 = self.rotate_90(state, action)\n",
    "        #rotate by 180 degrees\n",
    "        sa_rot180 = self.rotate_90(sa_rot90[0], sa_rot90[1])\n",
    "        #rotate by 270 degrees\n",
    "        sa_rot270 = self.rotate_90(sa_rot180[0], sa_rot180[1])\n",
    "        \n",
    "        return [sa_v, sa_h, sa_]\n",
    "    \n",
    "    def choose_action(self, state, available_actions):\n",
    "        n_actions = len(available_actions)\n",
    "        q_action = [self.q_values[(str(state), action)] for action in available_actions]\n",
    "        max_q_value = max(q_action)\n",
    "        optimal_action_indices = [i for i in range(n_actions) if q_action[i]==max_q_value]\n",
    "        action_index = np.random.choice(optimal_action_indices)\n",
    "        if np.random.uniform()<self.eps and len(available_actions)>1:\n",
    "            action_index = np.random.choice(np.delete([i for i in range(n_actions)], action_index))\n",
    "        action = available_actions[action_index]\n",
    "        return action\n",
    "    \n",
    "    def learn(self, old_state, reward, new_state, action):\n",
    "        #calculate all symmetric states\n",
    "        symmetric_state_action_pairs = self.calc_symmetries(old_state, action)\n",
    "        state_action_pairs = symmetric_state_action_pairs + [(old_state, action)]\n",
    "        #state_action_pairs = [(old_state, action)]\n",
    "        #calculate new q-value\n",
    "        available_actions = game.get_possible_actions(new_state)\n",
    "        q_old = self.q_values[(str(old_state), action)]\n",
    "        q_action = [self.q_values[(str(new_state), a)] for a in available_actions]\n",
    "        #if new state is terminal\n",
    "        if game.is_final_state(new_state):\n",
    "            q_max = 0\n",
    "        else:\n",
    "            q_max = max(q_action)\n",
    "        q_new = q_old + self.alpha*(reward + self.gamma*q_max - q_old)\n",
    "        #update q-values of all symmetric states\n",
    "        for state_action in state_action_pairs:\n",
    "            state_, action_ = state_action\n",
    "            if game.is_final_state(old_state):\n",
    "                raise ValueError(\"Updating final state!\")\n",
    "            self.q_values[(str(state_), action_)] = q_new\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 10000\n",
    "game = TicTacToe()\n",
    "player1 = QLearning_Agent(game, 0)\n",
    "player2 = Random_Agent()\n",
    "episodes = np.arange(0, n_episodes, 1, dtype=int)\n",
    "r1, r2=play_tictactoe(game, player1, player2, n_episodes=n_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(data, window_size):\n",
    "    mean = np.zeros(len(data))\n",
    "    for i in range(len(data)//window_size):\n",
    "        current_mean = np.sum(data[i*window_size:(i+1)*window_size])/window_size\n",
    "        mean[i*window_size:(i+1)*window_size] = current_mean\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea774c3b2f522c7578217aa5ac60325c",
     "grade": true,
     "grade_id": "cw3_tictactoe_agent",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code for Exercise 2 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# You should implement your Q-Learning agent and produce your average learning curve plot.\n",
    "# Do NOT delete this cell.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "episodes = np.arange(0, n_episodes, 1)\n",
    "y1=running_mean(r1, 20)\n",
    "y2=running_mean(r2, 20)\n",
    "\n",
    "fig=plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(\"111\")\n",
    "ax.set_title(\"Cumulative rewards of a Q-Learning agent playing against a random agent\")\n",
    "ax.plot(episodes, y1, label=\"Q-Learning Agent\")\n",
    "ax.plot(episodes, y2, label=\"Random Agent\")\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Cum. reward\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Optimal Policy (8 Marks)\n",
    "In this exercise, we will test your agent's policy to see whether it has learned an optimal policy.\n",
    "\n",
    "Please define a function `test_policy(state)` below, which takes a state and returns the action that your agent would take in that state. **This function should not re-train an agent every time it is called**, it should use the policy that was learned by the agent that you trained in the previous exercise.\n",
    "\n",
    "We will represent the state as a list of nine characters, either `'X'`, `'O'` or `''` for cells occupied by X, O, an neither player respectively. The list index corresponding to each of the nine grid-cells is as follows:\n",
    "\n",
    "<img src=\"images/tic_tac_toe_grid_indexes.png\" style=\"width: 200px;\"/>\n",
    "\n",
    "The integer your `test_policy(state)` function returns should be the index corresponding to the cell your agent would place its O symbol in.\n",
    "\n",
    "We will be testing a number of states. Among them is the following state:\n",
    "\n",
    "<img src=\"images/tic_tac_toe_question_state.png\" style=\"width: 200px;\"/>\n",
    "\n",
    "This state will be passed into your `test_policy(state)` function as the list `['O', 'X', '', '', 'X', '', '', 'O', '']`. If, for example, your agent's policy is to place its symbol in the top-right cell, your function should return the integer value `2`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e4f553db30f83b89cfe9b04658f2a28",
     "grade": false,
     "grade_id": "cw3_tic_tac_toe_optimal_policy",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code for Exercise 3 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# You should implement your test_policy(state) function here.\n",
    "# Do NOT delete this cell.\n",
    "# YOUR CODE HERE\n",
    "def test_policy(state) :\n",
    "\n",
    "    state_array = np.zeros((3,3))\n",
    "    for index, symbol in enumerate(state):\n",
    "        i,j = index//3, index%3\n",
    "        if symbol=='O':\n",
    "            state_array[i,j]=1\n",
    "        elif symbol=='X':\n",
    "            state_array[i,j]=-1\n",
    "        elif symbol=='':\n",
    "            state_array[i,j]=0\n",
    "        else:\n",
    "            raise ValueError(\"Invalid symbol in array:\",symbol)\n",
    "    available_actions = game.get_possible_actions(state_array)\n",
    "    print(state_array)\n",
    "    q_actions = [player1.q_values[(str(state_array), action)] for action in available_actions]\n",
    "    print(max(q_actions))\n",
    "    best_action_index = np.argmax(q_actions)\n",
    "    best_action = available_actions[best_action_index]\n",
    "    index = 3*best_action[0] + best_action[1]\n",
    "    return index    \n",
    "test_policy(['O','X','',\n",
    "            '','','',\n",
    "            '','',''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randint(0, 10, size=(3,3))\n",
    "print(a)\n",
    "print(a[-1::-1, -1::-1].transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f512db2ab299bdc1a1e19677a66b5f5",
     "grade": true,
     "grade_id": "cw3_tic_tac_toe_optimal_policy_test_1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "# Your code for Exercise 3 is tested here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e6894e95017470dcf3c37e2c7d1c1692",
     "grade": true,
     "grade_id": "cw3_tic_tac_toe_optimal_policy_test_2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "# Your code for Exercise 3 is tested here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9604e4ce8475bf9837276485ba9fffb5",
     "grade": true,
     "grade_id": "cw3_tic_tac_toe_optimal_policy_test_3",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "# Your code for Exercise 3 is tested here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e49ed4aa7054c0f57b60bf0a74474faa",
     "grade": true,
     "grade_id": "cw3_tic_tac_toe_optimal_policy_test_4",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "# Your code for Exercise 3 is tested here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "13e65d6473a729d1937d83d45a469d74",
     "grade": false,
     "grade_id": "cell-5003798812b53f0c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 4: Discussion (32 Marks)\n",
    "\n",
    "In eight sentences or fewer, please discuss the following:\n",
    "- Your chosen state representation.\n",
    "- Any additions that you have made to your implementation beyond implementing basic Q-Learning.\n",
    "- The effects you expected your additions to have, and the extent to which your expectations were met.\n",
    "- Further modifications you believe may enhance the performance of your agent, or changes you would make if you had more time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb2d64b7664c050da9cb9d13ff54d42b",
     "grade": true,
     "grade_id": "cell-cw3_tic_tac_toe_discussion",
     "locked": false,
     "points": 32,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "For the task of teaching a Q-Learning agent to learn an optimal policy playing Tic-Tac-Toe against a random agent, the following state representation was used: The state representing the game status was a 3x3 array, with 0 representing an empty field, 1 standing for a field with a \"O\" on it and -1 representing a field with a \"X\" on it.\n",
    "\n",
    "In addition to the normal Q-Learning implementation following additions were made to the Q-Learning agent:\n",
    "Whenever a Q-value for a state-action pair was updated, the same update was performed to all symmetric state-value pairs. Tic-Tac-Toe states are symmetric with respect to horizontal and vertical reflections and rotations by 90, 180 and 270 degrees.\n",
    "So each time the Q-value of a specific state-action pair is updated, all the symmetric state-action pairs are calculated and their respective values in the Q-table are updated as well. \n",
    "\n",
    "My expectation was that this would lead to the Q-agent learning more quickly, because it allowed it to improve its policy for states that it did not explicitly experience during the training, or at least not as often. This expectation was met to some extent, as the reward accumulated by the agent increased earlier in the training, as opposed to it learning naively. However, as each time a state-action pair was updated its symmetries were explicitly calculated, this led to a dent in performance. It might have been better to save these state-action pairs once they had been calculated in a dictionary and simply look them up as it became necessary (leading to an increase in memory-usage).\n",
    "\n",
    "Further modifications, which might have improved the agent's learning performance: Identifying state-action pairs resulting in the same so-called afterstate and simultaneously updating their Q-values would have also lead to an increase in the performance of the agent, but this was not implemented in this version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
