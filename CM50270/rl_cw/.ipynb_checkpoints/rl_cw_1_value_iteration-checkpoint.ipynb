{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4610d145940e7891e7fd2b6582ff784",
     "grade": false,
     "grade_id": "cell-33b0e4dce2016840",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# CM50270 Reinforcement Learning\n",
    "## Coursework Part 1: Value Iteration\n",
    "\n",
    "In this exercise, you will implement the value iteration algorithm for three closely related, but different, gridworld environments.\n",
    "\n",
    "**Total number of marks:** 30 marks.\n",
    "\n",
    "**What to submit:** Your completed Jupyter notebook (.ipynb file) which should include all of your source code. Please do not change the file name or compress/zip your submission. Please do not include any identifying information on the files you submit. This coursework will be **marked anonymously**.\n",
    "\n",
    "**Where to submit:** CM50270 Moodle Page.\n",
    "\n",
    "You are required to **work individually**. You are welcome to discuss ideas with others but you must design your own implementation and write your own code.\n",
    "\n",
    "**Do not plagiarise**. Plagiarism is a serious academic offence. For details on what plagiarism is and how to avoid it, please visit the following webpage: http://www.bath.ac.uk/library/help/infoguides/plagiarism.html\n",
    "\n",
    "If you are asked to use specific variable names, data-types, function signatures and notebook cells, **please ensure that you follow these instructions**. Not doing so will cause the automarker to reject your work, and will assign you a score of zero for that question. **If the automarker rejects your work because you have not followed the instructions, you may not get any credit for your work**.\n",
    "\n",
    "Please remember to **save your work regularly**.\n",
    "\n",
    "Please be sure to **restart the kernel and run your code from start-to-finish** (Kernel â†’ Restart & Run All) before submitting your notebook. Otherwise, you may not be aware that you are using variables in memory that you have deleted.\n",
    "\n",
    "Your total runtime must be less than **1 minute** on the University's computers in 10W 0.02. Otherwise, you may not get credit for your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f9cbda0945ccab26f2a88ea824ace6a",
     "grade": false,
     "grade_id": "cell-49e38b6d0da7d1fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introduction\n",
    "In this coursework, you will implement the Value Iteration algorithm to compute an optimal policy for three different (but closely related) Markov Decision Processes. For your reference, the pseudo-code for the Value Iteration algorithm is reproduced below from the textbook (Reinforcement Learning, Sutton & Barto, 2018, pp. 83).\n",
    "\n",
    "<img src=\"images/value_iteration.png\" style=\"width: 800px;\"/>\n",
    "\n",
    "Please note the following about the pseudo-code: The set $\\mathcal{S}$ contains all non-terminal states, whereas $\\mathcal{S}^+$ is the set of all states (terminal and non-terminal). The symbol $r$ represents the immediate reward on transition from state $s$ to the next state $s'$ via action $a$. \n",
    "\n",
    "<img src=\"images/bombs and gold numbers.png\" style=\"width: 300px;\" align=\"left\" caption=\"Figure 1\"/>\n",
    "\n",
    "The three problems you will solve use variants of the gridworld environment shown on the left. You should be familiar with this kind of environment from the lectures. The grid squares in the figure are numbered as shown. In all exercises, the following are true: \n",
    "\n",
    "**Actions available:** The agent has four possible actions in each grid square. These are *west*, *north*, *south*, and *east*. If the direction of movement is blocked by a wall (for example, if the agent executes action south at grid square 1), the agent remains in the same grid square. \n",
    "\n",
    "**Collecting gold:** On its first arrival at a grid square that contains gold (from a neighbouring grid square), the agent collects the gold. Note that, in order to collect the gold, the agent needs to transition into the grid square (containing the gold) from a different grid square.\n",
    "\n",
    "**Hitting the bomb:** On arrival at a grid square that contains a bomb (from a neighbouring grid square), the agent activates the bomb. \n",
    "\n",
    "**Terminal states:** The game terminates when all gold is collected or when the bomb is activated. In Exercises 1 and 2, you can define terminal states to be grid squares 18 and 23. In Exercise 3, you will need to define terminal state(s) differently.\n",
    "\n",
    "\n",
    "### Instructions ###\n",
    "Set parameter $\\theta$ to 1 to the power of -10. You can express that as `1e-10` in Python. \n",
    "\n",
    "Set all initial state values $V(s)$ to zero.\n",
    "\n",
    "Do not use discounting (that is, set $\\gamma=1$).\n",
    "\n",
    "Use the following reward function: $-1$ for each navigation action (including when the action results in hitting the wall), an additional $+10$ for collecting each piece of gold, and an additional $-10$ for activating the bomb. For example, the immediate reward for transitioning into a square with gold (from a neighbouring grid square) is $-1 + 10 = +9$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e2dd22961d7366a201a71ad91af57c5d",
     "grade": false,
     "grade_id": "cell-bb45c706447879a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 1: Deterministic Environment (5 Marks)\n",
    "\n",
    "In this exercise, the agent is able to move in the intended direction with certainty. For example, if it executes action _north_ in grid square 0, it will transition to grid square 5 with probability 1. In other words, we have a deterministic environment.\n",
    "\n",
    "Compute the optimal policy using Value Iteration. \n",
    "\n",
    "Your need to produce two one-dimensional numpy arrays with names `policy` and `v`. Both arrays should have a length of 25, with the element at index $i$ representing grid cell $i$ (see figure above). Both arrays should be accessible in the \"solution cell\" below!\n",
    "\n",
    "The array `policy` should be a numpy array of strings that specifies an optimal action at each grid location. Please use the abbreviations \"n\", \"e\", \"s\", and \"w\" for the four actions. As an example, the value of `policy` at index `0` needs to give `\"n\"`, if _north_ is an optimal action in cell 0. The policy for a terminal state can be any action. If there are multiple optimal actions from a state, any optimal action will be considered as a correct answer. \n",
    "\n",
    "The array `v` should be an array of floats that contains the expected return at each grid square (that is, the state value under the optimal policy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    \n",
    "    def __init__(self, nCols=5, nRows=5, random_move_prob=0.2, bomb_loc=(18, ), gold_loc=(23, )):\n",
    "        \n",
    "        #Initialise values that define the grid and some other essential parameters\n",
    "        self.nCols = nCols\n",
    "        self.nRows = nRows\n",
    "        if self.nRows<2 or self.nCols<2:\n",
    "            raise ValueError(\"Number of columns and rows may not be smaller than 2\")\n",
    "        self.nCells = nCols*nRows\n",
    "        self.random_move_prob = random_move_prob\n",
    "        self.bomb = np.array(bomb_loc)\n",
    "        self.gold = np.array(gold_loc)\n",
    "         \n",
    "        #Set reward array\n",
    "        self.reward = np.zeros(self.nCells)\n",
    "        self.reward[self.gold] = 10\n",
    "        self.reward[self.bomb] = -10\n",
    "        \n",
    "        #Define possible actions\n",
    "        self.actions = [\"n\", \"e\", \"s\", \"w\"]\n",
    "        self.nActions = len(self.actions)\n",
    "        \n",
    "    def getBellmanValues(self, state, action):\n",
    "        transition_probs = np.zeros(self.nActions)\n",
    "        rewards = np.zeros_like(transition_probs)\n",
    "        new_states = np.zeros_like(transition_probs, dtype=int)\n",
    "        for i, a in enumerate(self.actions):\n",
    "            new_pos = state\n",
    "            if a == \"s\":\n",
    "                proposed_pos = state - self.nCols\n",
    "                if proposed_pos>=0:\n",
    "                    new_pos = proposed_pos\n",
    "            elif a == \"n\":\n",
    "                proposed_pos = state + self.nCols\n",
    "                if proposed_pos<self.nCells:\n",
    "                    new_pos = proposed_pos\n",
    "            elif a == \"w\":\n",
    "                proposed_pos = state - 1\n",
    "                if state%self.nCols != 0:\n",
    "                    new_pos = proposed_pos\n",
    "            elif a == \"e\":\n",
    "                proposed_pos = state + 1\n",
    "                if state%self.nCols != self.nCols - 1:\n",
    "                    new_pos = proposed_pos\n",
    "            \n",
    "            new_states[i] = new_pos\n",
    "            if a==action:\n",
    "                transition_probs[i]=1 - self.random_move_prob + self.random_move_prob/(self.nActions)\n",
    "            else:\n",
    "                transition_probs[i]=self.random_move_prob/(self.nActions)\n",
    "\n",
    "            rewards[i] = self.reward[new_pos]\n",
    "            rewards[i] += -1\n",
    "            \n",
    "        return transition_probs, rewards, new_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma, theta=1e-10):\n",
    "    delta = 1\n",
    "    n_states = env.nCells\n",
    "    n_actions = env.nActions\n",
    "    bomb = env.bomb\n",
    "    gold = env.gold\n",
    "    value_array = np.zeros(n_states)\n",
    "    policy_array = np.array(['n' for i in range(n_states)])\n",
    "    nCols = env.nCols\n",
    "    nRows = env.nRows\n",
    "    index = 0\n",
    "    while delta > theta:\n",
    "        delta = 0\n",
    "        for state in range(n_states):\n",
    "            if state not in (*gold, *bomb):\n",
    "                value_current = value_array[state]\n",
    "                bellman_sums = np.zeros(n_actions)\n",
    "                \n",
    "                for action_index in range(n_actions):\n",
    "                    action = env.actions[action_index]\n",
    "                    transition_probs, expected_rewards, new_states = env.getBellmanValues(state, action)\n",
    "                    bellman_sums[action_index] = np.sum(transition_probs*(expected_rewards + gamma*value_array[new_states]))\n",
    "        \n",
    "                value_array[state] = np.max(bellman_sums)\n",
    "                delta = np.max([delta, np.abs(value_current - value_array[state])])\n",
    "                best_action_index=np.argmax(bellman_sums)\n",
    "                best_action = env.actions[best_action_index]\n",
    "                policy_array[state] = best_action\n",
    "                \n",
    "    return value_array, policy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d977787eb382a68d34590d73a53bc2d",
     "grade": false,
     "grade_id": "cw1_value_iteration_deterministic",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code for Exercise 1 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# Your code should compute the values of policy and v from scratch when this cell is executed, \n",
    "# using the value iteration algorithm. We will mark your coursework by checking the values of \n",
    "# the variables policy and v in the hidden test cell further below. Do NOT delete this cell.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "env = GridWorld(random_move_prob=0) #deterministic gridworld environment\n",
    "v, policy = value_iteration(env, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "522fe97cfe6a496af1f174947eef91ab",
     "grade": false,
     "grade_id": "cell-41064d5bad0ebfbf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Check the data types of your solution! \n",
    "Below, we have provided a number of example tests which allow you to check whether the variables `policy` and `v` (which you should have computed *above* this cell) have the correct data-types. Feel free to re-use these tests to check the data-types for the remaining exercises, and construct your own similar tests for later pieces of coursework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example arrays with CORRECT data types (but WRONG values!) would be:\n",
    "# policy = np.array([\"n\", \"w\", \"s\", \"w\", \"e\", \"n\", \"w\", \"s\", \"w\", \"e\", \n",
    "#                    \"n\", \"w\", \"s\", \"w\", \"e\", \"n\", \"w\", \"s\", \"w\", \"e\", \n",
    "#                    \"n\", \"w\", \"s\", \"w\", \"e\"])\n",
    "# v = np.random.rand(25)\n",
    "# DO NOT UNCOMMENT THE PREVIOUS lines in your submission... otherwise they will overwrite the arrays that you computed!\n",
    "\n",
    "# Print the values you computed\n",
    "print(\"This is your 'policy':\")\n",
    "print(policy)\n",
    "print(\"These are your state values 'v':\")\n",
    "print(v)\n",
    "\n",
    "# Check whether both policy and v are numpy arrays.\n",
    "import numpy as np\n",
    "assert(isinstance(policy, np.ndarray))\n",
    "assert(isinstance(v, np.ndarray))\n",
    "\n",
    "# Check correct shapes of numpy arrays.\n",
    "assert(policy.shape == (25, ))\n",
    "assert(v.shape == (25, ))\n",
    "\n",
    "# Check whether the numpy arrays have the correct data types.\n",
    "assert(np.issubdtype(policy.dtype, np.unicode_)) # policy.dtype should be '<U1'\n",
    "assert(np.issubdtype(v.dtype, np.float64))\n",
    "\n",
    "# Check whether all policy values are either \"n\", \"w\", \"s\", or \"e\".\n",
    "assert(np.all(np.isin(policy, np.array([\"n\", \"w\", \"s\", \"e\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "401d440467c321ba8b359bfa71764f3f",
     "grade": true,
     "grade_id": "cw1_value_iteration_deterministic_tests",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "# Your code for Exercise 1 is tested here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "45ec4877999aaf6b5e6c4d393edb5481",
     "grade": false,
     "grade_id": "cell-05eb78b7446cb694",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2: Stochastic Environment (10 Marks)\n",
    "\n",
    "In this exercise, we introduce stochasticity into the environment. Now, the agent is not always able to execute its actions as intended.\n",
    "\n",
    "With probability 0.8, the agent moves as intended. However, with probability 0.2, it moves in a random direction.\n",
    "\n",
    "For example, from grid square 0, if the agent tries to move north, with probability 0.8 the action will work as intended. But with probability 0.2, the agent's motor control system will move it in a random direction (including north). So, it will randomly try to move west, east, north or south with probability 0.05 each. Notice that the total probability of moving to square 5 (as intended) is 0.8 + 0.05 = 0.85.\n",
    " \n",
    "Compute the optimal policy using Value Iteration.\n",
    "\n",
    "Your value iteration method should output two one-dimensional numpy arrays with names `policy` and `v`, as in Exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49dee915875c20c51688b1c755dd633a",
     "grade": false,
     "grade_id": "cw1_value_iteration_stochastic",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code for Exercise 2 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# Your code should compute the values of policy and v from scratch when this cell is executed, \n",
    "# using the value iteration algorithm. We will mark your coursework by checking the values of \n",
    "# the variables policy and v in the hidden test cell further below. Do NOT delete this cell.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "env_stochastic = GridWorld(random_move_prob=0.2)\n",
    "v, policy = value_iteration(env_stochastic, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0a016f9d6ebbb7962084aef288fdba4",
     "grade": true,
     "grade_id": "cw1_value_iteration_stochastic_tests",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "# Your code for Exercise 2 is tested here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Stochastic Environment with Two Pieces of Gold (15 marks)\n",
    "\n",
    "<img src=\"images/bomb and two gold.png\" style=\"width: 300px;\" align=\"left\" caption=\"Figure 1\"/> In this exercise, we have modified the stochastic environment presented in exercise 2. A second piece of gold has been placed on grid square 12. The terminal state is reached only when _all_ pieces of gold are collected or when the bomb is activated.\n",
    "\n",
    "Compute the optimal policy for this altered environment using Value Iteration.\n",
    "\n",
    "Hint: You will need to change your state representation in order to account for the additional piece of gold.\n",
    "\n",
    "Your method should output two one-dimensional numpy arrays with names `policy` and `v`, as in the previous exercises. These arrays should specify the expected return and an optimal policy at the corresponding grid sqaure **before any pieces of gold are collected or a bomb is activated.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld_NGold(object):\n",
    "    \n",
    "    def __init__(self, columns=5, rows=5, gold_loc=(23, 12), bomb_loc=(18,), random_step_probability=0.2):\n",
    "        self.cols = columns\n",
    "        self.rows = rows\n",
    "        self.cells = columns * rows\n",
    "        self.gold = len(gold_loc)\n",
    "        self.gold_loc = gold_loc\n",
    "        self.bomb_loc = bomb_loc\n",
    "        self.rand_step_prob = random_step_probability\n",
    "        \n",
    "        self.loc_states = [i for i in range(self.cells)]\n",
    "        self.gold_states = self.calc_gold_permutations()\n",
    "        self.states = list(itertools.product(self.loc_states, self.gold_states))\n",
    "        self.final_states = [state for state in self.states if len(state[1]) == self.gold\n",
    "                            or state[0] in bomb_loc]\n",
    "        \n",
    "        self.actions = [\"n\", \"w\", \"s\", \"e\"]\n",
    "        self.nActions = len(self.actions)\n",
    "        \n",
    "    def calc_gold_permutations(self):\n",
    "        n = len(self.gold_loc)\n",
    "        permutations=[list(itertools.combinations([i for i in range(n)], j)) for j in range(n+1)]\n",
    "        gold_states = []\n",
    "        for i,permutation in enumerate(permutations):\n",
    "            if len(permutation)>=1:\n",
    "                for gold in permutation:\n",
    "                    gold_states.append(list(gold))\n",
    "        return gold_states\n",
    "    \n",
    "    def calc_new_location(self, old_location, action):\n",
    "        new_pos = old_location\n",
    "        if action == \"s\":\n",
    "            proposed_pos = old_location - self.cols\n",
    "            if proposed_pos>=0:\n",
    "                new_pos = proposed_pos\n",
    "        elif action == \"n\":\n",
    "            proposed_pos = old_location + self.cols\n",
    "            if proposed_pos<self.cells:\n",
    "                new_pos = proposed_pos\n",
    "        elif action == \"w\":\n",
    "            proposed_pos = old_location - 1\n",
    "            if old_location%self.cols != 0:\n",
    "                new_pos = proposed_pos\n",
    "        elif action == \"e\":\n",
    "            proposed_pos = old_location + 1\n",
    "            if old_location%self.cols != self.cols - 1:\n",
    "                new_pos = proposed_pos  \n",
    "        return new_pos\n",
    "        \n",
    "    def calc_prob(self, old_location, old_gold, new_gold, action, alt_action):\n",
    "        old_loc = old_location\n",
    "        prop_loc = self.calc_new_location(old_loc, alt_action)\n",
    "        new_loc = self.calc_new_location(old_loc, action)\n",
    "        \n",
    "        if action == alt_action:\n",
    "            prob = 1 - self.rand_step_prob + self.rand_step_prob/self.nActions\n",
    "        else:\n",
    "            prob = self.rand_step_prob/self.nActions\n",
    "        \n",
    "        isGold = new_loc in self.gold_loc\n",
    "        if not isGold:\n",
    "            if new_gold != old_gold:\n",
    "                prob = 0\n",
    "        else:\n",
    "            gold_index = self.gold_loc.index(new_loc)\n",
    "            if gold_index in old_gold:\n",
    "                if set(old_gold)!=set(new_gold):\n",
    "                    prob = 0\n",
    "            \n",
    "            elif len(new_gold) > len(old_gold)+1:\n",
    "                prob = 0\n",
    "            elif set(old_gold +  [gold_index]) != set(new_gold):\n",
    "                prob = 0\n",
    "                \n",
    "        return prob   \n",
    "    \n",
    "    def calc_reward(self, old_location, old_gold, action):\n",
    "        old_loc = old_location\n",
    "        new_loc = self.calc_new_location(old_loc, action)\n",
    "        reward = -1\n",
    "        if new_loc in self.bomb_loc:\n",
    "            reward -= 10\n",
    "        elif new_loc in self.gold_loc:\n",
    "            gold_index = self.gold_loc.index(new_loc)\n",
    "            if gold_index not in old_gold:\n",
    "                reward += 10\n",
    "    \n",
    "        return reward \n",
    "    \n",
    "    def calc_bellman_values(self, old_location, old_gold, action):\n",
    "        n = len(self.gold_states)*self.nActions # number of possible new states following action\n",
    "        new_states = [(None,None) for i in range(n)]\n",
    "        transit_probs = np.zeros(n)\n",
    "        rewards = np.zeros(n)\n",
    "        state_indices = np.zeros(n, dtype=int)\n",
    "        old_loc = old_location\n",
    "        index = 0\n",
    "        \n",
    "        for alt_action in self.actions:\n",
    "            alt_loc = self.calc_new_location(old_loc, alt_action)\n",
    "            for new_gold in self.gold_states:\n",
    "                reward = self.calc_reward(old_location, old_gold, alt_action)\n",
    "                rewards[index] = reward\n",
    "                transit_probs[index] = self.calc_prob(old_loc, old_gold, new_gold, action, alt_action)\n",
    "                new_states[index]=(alt_loc, new_gold)\n",
    "                state_indices[index]=self.states.index((alt_loc, new_gold))\n",
    "                index+=1\n",
    "                \n",
    "        return transit_probs, rewards, state_indices       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_NGold(env, gamma, theta=1e-10):\n",
    "    delta = 1\n",
    "    n_loc = env.cells\n",
    "    n_gold = len(env.gold_states)\n",
    "    states = env.states\n",
    "    n_states = len(states)\n",
    "    n_actions = env.nActions\n",
    "    value_array = np.zeros(n_states)\n",
    "    policy_array = [\"n\" for i in range(n_states)]\n",
    "    \n",
    "    while delta > theta:\n",
    "        delta = 0\n",
    "        for i, state in enumerate(states):\n",
    "            loc, gold = state\n",
    "            if state not in env.final_states:\n",
    "                value_current = value_array[i]\n",
    "                bellman_sums = np.zeros(n_actions)\n",
    "                for action_index in range(n_actions):\n",
    "                    action = env.actions[action_index]\n",
    "                    transition_probs, expected_rewards, state_indices = env.calc_bellman_values(loc, gold, action)\n",
    "                    values = value_array[state_indices]\n",
    "                    bellman_sums[action_index] = np.sum(transition_probs*(expected_rewards + gamma*values))\n",
    "                   \n",
    "                value_array[i] = np.max(bellman_sums)\n",
    "                delta = np.max([delta, np.abs(value_current - value_array[i])])\n",
    "                best_action_index=np.argmax(bellman_sums)\n",
    "                best_action = env.actions[best_action_index]\n",
    "                policy_array[i] = best_action\n",
    "                \n",
    "    return value_array, policy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39a687ebcc5248cc934f6aa359670f65",
     "grade": false,
     "grade_id": "cw1_value_iteration_two_gold",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code for Exercise 3 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# Your code should compute the values of policy and v from scratch when this cell is executed, \n",
    "# using the value iteration algorithm. We will mark your coursework by checking the values of \n",
    "# the variables policy and v in the hidden test cell further below. Do NOT delete this cell.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "env_NGold = GridWorld_NGold(gold_loc=(23, 12), random_step_probability=0.2)\n",
    "value_array, policy_array = value_iteration_NGold(env_NGold, 1.)\n",
    "states_0 = [state for state in env_NGold.states if state[1]==[]] #States with no gold collected yet\n",
    "state_0_indices = [env_NGold.states.index(state) for state in states_0] #Indices of states with no gold collected yet\n",
    "\n",
    "policy = [policy_array[index] for index in state_0_indices] #policy for states with no collected gold\n",
    "v = [value_array[index] for index in state_0_indices] #value array for states with no collected gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "333118819607b9875a60b83d527ff541",
     "grade": true,
     "grade_id": "cw1_value_iteration_two_gold_tests",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL!\n",
    "# Your code for Exercise 3 is tested here.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
