{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    def __init__(self):\n",
    "        #basic properties of the grid\n",
    "        self.rows = int(5)\n",
    "        self.cols = int(5)\n",
    "        self.num_cells = self.rows * self.cols\n",
    "        self.rand_move_prob = 0.2\n",
    "        \n",
    "        #choose intial position of the agent randomly within the first quadrant\n",
    "        self.init_position = [np.random.randint(0, self.cols*0.25), np.random.randint(0, self.rows*0.25)]\n",
    "        self.agent_position = np.asarray(self.init_position)\n",
    "        \n",
    "        #choose position of the bomb and of the gold\n",
    "        self.bomb_position = np.asarray([3, 3])\n",
    "        self.gold_position = np.asarray([3, 4])\n",
    "        \n",
    "        #set up reward matrix\n",
    "        self.rewards = np.zeros(shape=(self.rows, self.cols))\n",
    "        self.rewards[tuple(self.bomb_position)] = -10\n",
    "        self.rewards[tuple(self.gold_position)] = 10\n",
    "        \n",
    "        #define possible actions\n",
    "        self.actions = [\"LEFT\", \"RIGHT\", \"UP\", \"DOWN\"]\n",
    "        self.num_actions = len(self.actions)\n",
    "    \n",
    "    def get_available_actions(self):\n",
    "        return self.actions\n",
    "        \n",
    "    def draw_internal_state(self):\n",
    "        #print a string depicting the current internal state\n",
    "        string=''\n",
    "        for j in range(self.rows):\n",
    "            for i in range(self.cols):\n",
    "                pos = (i, j)\n",
    "                if np.array_equal(pos, self.agent_position):\n",
    "                    string+=\"O\"\n",
    "                elif np.array_equal(pos, self.bomb_position):\n",
    "                    string+='B'\n",
    "                elif np.array_equal(pos, self.gold_position):\n",
    "                    string+='G'\n",
    "                else:\n",
    "                    string+=\"#\"\n",
    "            string+=\"\\n\"\n",
    "        \n",
    "        string+=\"\\n\"\n",
    "        #string+=\"O-Position of Agent\\t B-Position of Bomb\\t G-Position of Gold\"\n",
    "        return string \n",
    "    \n",
    "    def make_step(self, action_index):\n",
    "        #Roll a dice and see if agent does a random move instead of the intended move\n",
    "        if np.random.uniform(0,1) < self.rand_move_prob:\n",
    "            action_indices = np.arange(self.num_actions, dtype=\"int\")\n",
    "            action_indices = np.delete(action_indices, action_index)\n",
    "            action_index = np.random.choice(action_indices, 1)[0]     \n",
    "        #Check if the agent hits a wall\n",
    "        action = self.actions[action_index]\n",
    "        if action == \"UP\":\n",
    "            candidate_position = self.agent_position + [0, 1]\n",
    "            new_position = candidate_position\n",
    "            if candidate_position[1]==self.rows:\n",
    "                new_position = self.agent_position\n",
    "        elif action == \"DOWN\":\n",
    "            candidate_position = self.agent_position + [0, -1]\n",
    "            new_position = candidate_position\n",
    "            if candidate_position[1]==-1:\n",
    "                new_position = self.agent_position\n",
    "        elif action == \"LEFT\":\n",
    "            candidate_position = self.agent_position + [-1, 0]\n",
    "            new_position = candidate_position\n",
    "            if candidate_position[0]==-1:\n",
    "                new_position = self.agent_position\n",
    "        elif action == \"RIGHT\":\n",
    "            candidate_position = self.agent_position + [1, 0]\n",
    "            new_position = candidate_position\n",
    "            if candidate_position[0]==self.cols:\n",
    "                new_position = self.agent_position\n",
    "        \n",
    "        self.agent_position = new_position\n",
    "        reward = self.rewards[tuple(self.agent_position)]\n",
    "        reward+=-1\n",
    "        return reward, new_position\n",
    "    \n",
    "    def reset(self):\n",
    "        self.agent_position = np.asarray(self.init_position)\n",
    "        \n",
    "    def isFinalState(self):\n",
    "        if np.array_equal(self.agent_position, self.bomb_position) or np.array_equal(self.agent_position, self.gold_position):\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent():\n",
    "    def choose_action(self, available_actions):\n",
    "        number_of_actions = len(available_actions)\n",
    "        random_action_index = np.random.randint(0, number_of_actions)\n",
    "        return random_action_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 0\n",
    "rand_reward_array=[]\n",
    "env = GridWorld()\n",
    "agr = RandomAgent()\n",
    "available_actions = env.actions\n",
    "k=0\n",
    "while k<500:\n",
    "    action_index=agr.choose_action(available_actions)\n",
    "    rew, pos = env.make_step(action_index)\n",
    "    value += rew\n",
    "    if env.isFinalState():\n",
    "        env.reset()\n",
    "        rand_reward_array.append(value)\n",
    "        value=0\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent(object):\n",
    "    def choose_action(self, available_actions, Q_table, state_index, eps):\n",
    "        number_of_actions = len(available_actions)\n",
    "        statex, statey = state_index\n",
    "        if np.random.uniform(0, 1)<eps:\n",
    "            action_index=np.random.randint(0, number_of_actions)\n",
    "        else:\n",
    "            max_Q = np.max(Q_table[statex, statey])\n",
    "            max_indices=np.nonzero(Q_table[statex, statey]==max_Q)[0]\n",
    "            action_index = np.random.choice(max_indices)\n",
    "        return action_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "agQ = QAgent()\n",
    "available_actions = env.actions\n",
    "Q_table = np.zeros((5,5,4))\n",
    "alpha = 0.1 #learning rate\n",
    "gamma = 1 #discounted return factor\n",
    "value = 0\n",
    "k = 0\n",
    "step =0\n",
    "reward_array = []\n",
    "while k<500:\n",
    "    game_over=False\n",
    "    while step<1000 and not game_over:\n",
    "        statex, statey = env.agent_position\n",
    "        action_index=agQ.choose_action(available_actions, Q_table, state, 0.05)\n",
    "        rew, [new_statex, new_statey] = env.make_step(action_index)\n",
    "        value +=rew\n",
    "        step +=1\n",
    "        if env.isFinalState():\n",
    "            game_over=True\n",
    "        current_q_value = Q_table[statex, statey, action_index]\n",
    "        max_q_value_in_new_state = np.max(Q_table[new_statex, new_statey])\n",
    "        Q_table[statex, statey, action_index] = (1 - alpha) * current_q_value + alpha * (rew + gamma * max_q_value_in_new_state)\n",
    "    k+=1  \n",
    "    reward_array.append(value)\n",
    "    value=0\n",
    "    step=0\n",
    "    env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_array)\n",
    "#plt.plot(rand_reward_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
