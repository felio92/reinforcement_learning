{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution for RL lab 1: The agent-environment interaction \n",
    "\n",
    "No extra functionality, most parameters are hard-coded (instead of being defined as variables).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# For plotting of learning curves\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Gridworld class. \n",
    "\n",
    "Gold, bombs, and agent have default positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld:\n",
    "    def __init__(self):\n",
    "        self.num_rows = 5\n",
    "        self.num_cols = 5\n",
    "        self.gold_reward = 10\n",
    "        self.bomb_reward = -10\n",
    "        self.gold_positions = np.array([23])\n",
    "        self.bomb_positions = np.array([18])\n",
    "        self.random_move_probability = 0.2\n",
    "\n",
    "        self.actions = [\"UP\", \"RIGHT\", \"DOWN\", \"LEFT\"]\n",
    "        self.num_actions = len(self.actions)\n",
    "        self.num_fields = self.num_cols * self.num_rows\n",
    "        \n",
    "        self.rewards = np.zeros(shape=self.num_fields)\n",
    "        self.rewards[self.bomb_positions] = self.bomb_reward\n",
    "        self.rewards[self.gold_positions] = self.gold_reward\n",
    "\n",
    "        self.step = 0\n",
    "        self.cumulative_reward = 0\n",
    "        self.agent_position = np.random.randint(0, 5)\n",
    "\n",
    "    def make_step(self, action_index):\n",
    "        \"\"\"\n",
    "        Given an action, make state transition and observe reward.\n",
    "\n",
    "        :param action_index: an integer between 0 and the number of actions (4 in Gridworld).\n",
    "        :return: (reward, new_position)\n",
    "            WHERE\n",
    "            reward (float) is the observed reward\n",
    "            new_position (int) is the new position of the agent\n",
    "        \"\"\"\n",
    "        # Randomly sample action_index if world is stochastic\n",
    "        if np.random.uniform(0, 1) < self.random_move_probability:\n",
    "            action_indices = np.arange(self.num_actions, dtype=int)\n",
    "            action_indices = np.delete(action_indices, action_index)\n",
    "            action_index = np.random.choice(action_indices, 1)[0]\n",
    "\n",
    "        action = self.actions[action_index]\n",
    "\n",
    "        # Determine new position and check whether the agent hits a wall.\n",
    "        old_position = self.agent_position\n",
    "        new_position = self.agent_position\n",
    "        if action == \"UP\":\n",
    "            candidate_position = old_position + self.num_cols\n",
    "            if candidate_position < self.num_fields:\n",
    "                new_position = candidate_position\n",
    "        elif action == \"RIGHT\":\n",
    "            candidate_position = old_position + 1\n",
    "            if candidate_position % self.num_cols > 0:  # The %-operator denotes \"modulo\"-division.\n",
    "                new_position = candidate_position\n",
    "        elif action == \"DOWN\":\n",
    "            candidate_position = old_position - self.num_cols\n",
    "            if candidate_position >= 0:\n",
    "                new_position = candidate_position\n",
    "        elif action == \"LEFT\":  # \"LEFT\"\n",
    "            candidate_position = old_position - 1\n",
    "            if candidate_position % self.num_cols < self.num_cols - 1:\n",
    "                new_position = candidate_position\n",
    "        else:\n",
    "            raise ValueError('Action was mis-specified!')\n",
    "\n",
    "        # Update the environment state\n",
    "        self.agent_position = new_position\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = self.rewards[self.agent_position]\n",
    "        reward -= 1\n",
    "        return reward, new_position\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_position = np.random.randint(0, 5)\n",
    "\n",
    "    def is_terminal_state(self):\n",
    "        # The following statement returns a boolean. It is 'True' when the agent_position\n",
    "        # coincides with any bomb_positions or gold_positions.\n",
    "        return self.agent_position in np.append(self.bomb_positions, self.gold_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random agent and Q-learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent():\n",
    "    def __init__(self, environment):\n",
    "        self.environment = environment\n",
    "\n",
    "    def choose_action(self):\n",
    "        action = np.random.randint(0, self.environment.num_actions)\n",
    "        return action\n",
    "\n",
    "\n",
    "class AgentQ:\n",
    "    def __init__(self, environment, policy=\"epsilon_greedy\", epsilon=0.05, alpha=0.1, gamma=1):\n",
    "        self.environment = environment\n",
    "        # Initialize Q-table to zeros\n",
    "        self.q_table = np.zeros(shape=(self.environment.num_fields, self.environment.num_actions))\n",
    "        self.policy = policy\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def choose_action(self):\n",
    "        if self.policy == \"epsilon_greedy\" and np.random.uniform(0, 1) < self.epsilon:\n",
    "            action = np.random.randint(0, self.environment.num_actions)\n",
    "        else:\n",
    "            state = self.environment.agent_position\n",
    "            q_values_of_state = self.q_table[state, :]\n",
    "            # Choose randomly AMONG maximum Q-values\n",
    "            max_q_value = np.max(q_values_of_state)\n",
    "            maximum_q_values = np.nonzero(q_values_of_state == max_q_value)[0]\n",
    "            action = np.random.choice(maximum_q_values)\n",
    "        return action\n",
    "\n",
    "    def learn(self, old_state, reward, new_state, action):\n",
    "        max_q_value_in_new_state = np.max(self.q_table[new_state, :])\n",
    "        current_q_value = self.q_table[old_state, action]\n",
    "        self.q_table[old_state, action] = (1 - self.alpha) * current_q_value + self.alpha * (reward + self.gamma * max_q_value_in_new_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `play()` function\n",
    "\n",
    "This function lets the agent interact with the environment for a specified number of periods and returns the cumulative reward per episode. You _could_ also make this function a method of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(environment, agent, episodes=500, max_steps_per_episode=1000, learn=False):\n",
    "    \n",
    "    reward_per_episode = np.zeros(episodes)\n",
    "    \n",
    "    for episode in range(0, episodes):\n",
    "        environment.reset()\n",
    "        cumulative_reward = 0\n",
    "        step = 0\n",
    "        game_over = False\n",
    "        while step < max_steps_per_episode and not game_over:\n",
    "            old_state = environment.agent_position\n",
    "            action = agent.choose_action()\n",
    "            reward, new_state = environment.make_step(action)\n",
    "            if learn:\n",
    "                agent.learn(old_state, reward, new_state, action)\n",
    "            cumulative_reward += reward\n",
    "            step += 1\n",
    "            \n",
    "            # Check whether agent is at terminal state. If yes: end episode; reset agent.\n",
    "            if environment.is_terminal_state():\n",
    "                game_over = True\n",
    "                \n",
    "        reward_per_episode[episode] = cumulative_reward\n",
    "    return reward_per_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example learning curve: Random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment and agent\n",
    "environment = Gridworld()\n",
    "random_agent = RandomAgent(environment)\n",
    "\n",
    "reward_per_episode = play(environment, random_agent, episodes=500)\n",
    "\n",
    "# Simple learning curve\n",
    "plt.plot(reward_per_episode)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example learning curve: Q-learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "environment = Gridworld()\n",
    "agentQ = AgentQ(environment)\n",
    "\n",
    "# Note the learn=True argument!\n",
    "reward_per_episode = play(environment, agentQ, episodes=500, learn=True)\n",
    "\n",
    "# Simple learning curve\n",
    "plt.plot(reward_per_episode)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
